{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kurihara-ryo/cesarean-pasaran/blob/main/%E3%80%90%E8%A7%A3%E7%AD%94%E4%BE%8B%E3%80%91%E6%BC%94%E7%BF%92%E8%AA%B2%E9%A1%8C_%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0%E2%85%A2_1920.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#【課題】と書いてあるセルでは、自分でプログラムを作成してください。\n",
        "# 実行ボタンを押してプログラムの動作確認を行い、Google Classroomから提出してください。\n",
        "# 提出する際は、コメント(#から始まる部分)を削除せずに残してください。\n",
        "# 〆切が近いので、後回しにせず講義時間中に手を動かしてプログラムを作成すること。"
      ],
      "metadata": {
        "id": "Q5Oz_bNHEFRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 大規模データに対する深層学習\n",
        "# 大規模データの例として、Fashion MNISTデータセットを使用します。\n",
        "# 学習データ数50000、バリデーションデータ数10000、テストデータ数10000です。\n",
        "\n",
        "# 必要なモジュール一式を準備\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 乱数シードの設定\n",
        "seed = 42 # 変更しない\n",
        "torch.manual_seed(seed)\n",
        "print('Random seed: {}'.format(seed))\n",
        "\n",
        "# データセットの読み込み\n",
        "# torchvisionモジュールを使って、Fashion MNISTデータセットを読み込みます。\n",
        "# transformには、読み込みと同時に行う前処理を指定します。\n",
        "# ここではテンソルへの変換を行っています。\n",
        "# rootには、ダウンロードしたファイルの保存先ディレクトリを指定します。\n",
        "# 今回はGoogle Colab内に保存するので、適当なディレクトリでOKです。\n",
        "transform = transforms.ToTensor()\n",
        "train_val_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "print('\\n')\n",
        "\n",
        "# 学習データとバリデーションデータの分割\n",
        "# torchのrandom_split()を使って、train_val_datasetをランダムに分割します。\n",
        "# 今回は学習データ50000個、バリデーションデータ10000個に分割しています。\n",
        "train_dataset, val_dataset = random_split(train_val_dataset, [50000, 10000])\n",
        "\n",
        "# データ数を表示\n",
        "# テストデータは10000個です。\n",
        "print('Train data size: {}'.format(len(train_dataset)))\n",
        "print('Val data size: {}'.format(len(val_dataset)))\n",
        "print('Test data size: {}'.format(len(test_dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3sgdEIGflJ7",
        "outputId": "30d0333e-e0db-414b-b18c-d4e33c8b2711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed: 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 22.3MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 337kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.23MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 12.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Train data size: 50000\n",
            "Val data size: 10000\n",
            "Test data size: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの中身を確認します。\n",
        "# 各データは画像とラベルのペアからなります。\n",
        "# 画像はチャネル数1、高さ28ピクセル、幅28ピクセルの白黒画像を表すテンソル、\n",
        "# ラベルは10個のクラスどれかを表す整数(0, 1, ..., 9)です。\n",
        "# matplotlibで表示してみます。\n",
        "\n",
        "# 0番目のデータ\n",
        "image, label = train_val_dataset[0]\n",
        "# imageはshape (1, 28, 28)で、最初の次元はチャネル数\n",
        "# matplotlibで表示するため、最初の次元を削除してshape (28, 28)に変換\n",
        "image = image.squeeze(0)\n",
        "# imsshow()で画像をグレースケールで表示\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Label: {label}\")\n",
        "plt.show()\n",
        "\n",
        "# 5番目のデータも表示\n",
        "image, label = train_val_dataset[5]\n",
        "image = image.squeeze(0)\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Label: {label}\")\n",
        "plt.show()\n",
        "\n",
        "# 0番目の画像のラベルは9：Ankle boot(アンクルブーツ)\n",
        "# 5番目の画像のラベルは2：Pullover(プルオーバー、頭から被って着る服)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "1NUNG5tiqmJf",
        "outputId": "ed4e37b3-0389-4b6a-fe4e-c98913460717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10クラスのマルチクラス分類モデルを作成します。\n",
        "# まずは、シンプルな多層パーセプトロンを考えます。\n",
        "# 入力の画像データはshape (1, 28, 28)のテンソルですが、\n",
        "# 多層パーセプトロンで扱うデータは1次元テンソル(ベクトル)にする必要があります。\n",
        "# そのため以下の例では、ネットワーク構造の定義において、\n",
        "# nn.Flatten()を使ってテンソルを1次元に変換する処理を入れています。\n",
        "# それ以外の部分は第15,16回の演習課題と同様の流れです。\n",
        "\n",
        "# 乱数シードの設定\n",
        "torch.manual_seed(seed)\n",
        "print('Random seed: {}'.format(seed))\n",
        "\n",
        "# ネットワーク構造の定義\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),          # 1次元に変換\n",
        "    nn.Linear(28*28, 256), # (1, 28, 28)を1次元に変換したので、入力のユニット数は28*28\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)     # logitsを出力\n",
        ")\n",
        "\n",
        "# 損失関数の定義\n",
        "# マルチクラス分類用の交差エントロピー誤差nn.CrossEntropyLoss()を用いる。\n",
        "# 前回使用した2クラス分類用のnn.BCEWithLogitsLoss()のマルチクラス版。\n",
        "# クラス確率ではなくlogitsを引数とする点も同様。\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGDの定義\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# バッチサイズの指定\n",
        "# shuffle=Trueとすると、エポックごとにデータの順番をランダムに並び替える。\n",
        "# 学習後の汎化性能が良くなることがある。\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 学習ループ\n",
        "# データ数が多いため、10エポックでも計算時間かかる。\n",
        "for epoch in range(10):\n",
        "    model.train() # 学習モード：後で説明\n",
        "    epoch_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * X_batch.size(0)\n",
        "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
        "    print('Epoch {}, Loss: {}'.format(epoch, avg_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTByJyvpcLBe",
        "outputId": "c1c8d1f1-8262-4ef6-ccf2-ba0304247908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed: 42\n",
            "Epoch 0, Loss: 2.2722601040649413\n",
            "Epoch 1, Loss: 2.187280522918701\n",
            "Epoch 2, Loss: 2.0329482872009277\n",
            "Epoch 3, Loss: 1.7821661019897461\n",
            "Epoch 4, Loss: 1.512087251815796\n",
            "Epoch 5, Loss: 1.302744469833374\n",
            "Epoch 6, Loss: 1.1565939561843872\n",
            "Epoch 7, Loss: 1.0532730700874329\n",
            "Epoch 8, Loss: 0.9772792803001403\n",
            "Epoch 9, Loss: 0.9199367595291138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# テストデータに対するaccuracyを評価します。\n",
        "# マルチクラス分類のaccuracyを計算する場合、\n",
        "# accuracy_score()にはクラスラベルの整数が入ったリストを渡します。\n",
        "\n",
        "# accuracyの計算は何度も行うので関数にしておきます。\n",
        "# モデルとデータを引数として、accuracyを戻り値とする関数。\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval() # 評価モード：後で説明\n",
        "    y_true_list = []\n",
        "    y_pred_list = []\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            # logitsを計算\n",
        "            outputs = model(X)\n",
        "            # logitsの最も大きいクラスに割り当てる\n",
        "            # argmax()は最も大きい要素のインデックス(整数)を返す\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            # extend()はリストの連結を行う\n",
        "            y_true_list.extend(y.numpy())\n",
        "            y_pred_list.extend(preds.numpy())\n",
        "    acc = accuracy_score(y_true_list, y_pred_list)\n",
        "    return acc\n",
        "\n",
        "\n",
        "# テストデータのdataloaderを作成\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "# テストデータに対するaccuracyを評価\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print('Accuracy on test data: {}'.format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzsbZ90Tpvuv",
        "outputId": "78e73e58-298f-468a-949c-f948bdf04b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 0.6627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "【課題】上の例にならって、隠れ層1のユニット数を512、隠れ層2のユニット数を256に変更したネットワークを作成して、各エポックの損失関数の値を表示してください。学習後のモデルを使って、テストデータに対するaccuracyを計算して表示してください。それ以外の設定は上の例と同じにしてください。"
      ],
      "metadata": {
        "id": "WC30PNPyl4Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 乱数シードの設定\n",
        "torch.manual_seed(seed)\n",
        "print('Random seed: {}'.format(seed))\n",
        "\n",
        "# ネットワーク構造の定義\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(28*28, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 10)\n",
        ")\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGDの定義\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# バッチサイズの指定\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * X_batch.size(0)\n",
        "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
        "    print('Epoch {}, Loss: {}'.format(epoch, avg_loss))\n",
        "\n",
        "# テストデータのdataloaderを作成\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "# テストデータに対するaccuracyを評価\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print('Accuracy on test data: {}'.format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctr0-qjGl3J_",
        "outputId": "ac720fae-2b94-4a65-c490-1abc62f20b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed: 42\n",
            "Epoch 0, Loss: 2.2664272059631347\n",
            "Epoch 1, Loss: 2.1580254122161864\n",
            "Epoch 2, Loss: 1.9752341066741943\n",
            "Epoch 3, Loss: 1.70772761013031\n",
            "Epoch 4, Loss: 1.4419295090866089\n",
            "Epoch 5, Loss: 1.2404182201004028\n",
            "Epoch 6, Loss: 1.0996125386047364\n",
            "Epoch 7, Loss: 1.0016315516662597\n",
            "Epoch 8, Loss: 0.9315865235137939\n",
            "Epoch 9, Loss: 0.8796561162567139\n",
            "Accuracy on test data: 0.6707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### GPUの利用\n",
        "# GPUを使うことで計算を高速化できます。\n",
        "# Google Colabの右上のメニューから「ランタイムのタイプを変更」を選択して\n",
        "# ランタイムをT4 GPUに切り替えてください。\n",
        "# なお、Google Colabの無料アカウントではGPUの使用時間に上限があります。\n",
        "# 上限に到達してしまったら、ランタイムをCPUに戻しましょう。\n"
      ],
      "metadata": {
        "id": "9ugjsCHvn2dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ランタイムを切り替えたので、再度、初期設定を行います。\n",
        "\n",
        "# 必要なモジュール一式を準備\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 乱数シードの設定\n",
        "seed = 42 # 変更しない\n",
        "torch.manual_seed(seed)\n",
        "print('Random seed: {}'.format(seed))\n",
        "\n",
        "# データセットの読み込み\n",
        "transform = transforms.ToTensor()\n",
        "train_val_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# 学習データとバリデーションデータに分割\n",
        "train_dataset, val_dataset = random_split(train_val_dataset, [50000, 10000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6RfIH2O259M",
        "outputId": "9375fede-c2a2-4856-89c6-9d45a0660880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed: 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 13.2MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 210kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.90MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 8.49MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 以下のコードを実行すると、\n",
        "# GPUが使える場合には変数deviceの値が'cuda'になります。\n",
        "# GPUが使えない(CPUのみ使える)場合にはdeviceは'cpu'になります。\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: {}'.format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9l-1YdwQoH9",
        "outputId": "a855d952-9756-4e37-b54c-369075d17d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUを使用する場合、to(device)を使ってモデルをGPUに移動する必要があります。\n",
        "# また、モデルの計算に必要な変数も同様にGPUに移動する必要があります。\n",
        "# deviceが'cuda'ならGPU、deviceが'cpu'なら通常通りCPUを使用します。\n",
        "# そのため、自分でコードを書くときには最初からto(device)を入れておく方が便利です。\n",
        "\n",
        "# 乱数シードの設定\n",
        "torch.manual_seed(seed)\n",
        "print('Random seed: {}'.format(seed))\n",
        "\n",
        "# GPUを使えるか判定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: {}'.format(device))\n",
        "\n",
        "# ネットワーク構造の定義\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(28*28, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "## ↑deviceが'cuda'ならモデルをGPUに移動\n",
        "##   deviceが'cpu'なら通常通りCPUを使用\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGDの定義\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# バッチサイズの指定\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        ## X_batchはモデルの計算に必要なのでGPUに移動\n",
        "        ## y_batchも損失関数の計算に必要なのでGPUに移動\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * X_batch.size(0)\n",
        "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
        "    print('Epoch {}, Loss: {}'.format(epoch, avg_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYGcdhZn1lHf",
        "outputId": "9445cb62-ea7c-44ee-9755-9cb826f9884f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed: 42\n",
            "Using device: cpu\n",
            "Epoch 0, Loss: 2.2722601040649413\n",
            "Epoch 1, Loss: 2.187280522918701\n",
            "Epoch 2, Loss: 2.0329482872009277\n",
            "Epoch 3, Loss: 1.7821661019897461\n",
            "Epoch 4, Loss: 1.512087251815796\n",
            "Epoch 5, Loss: 1.302744469833374\n",
            "Epoch 6, Loss: 1.1565939561843872\n",
            "Epoch 7, Loss: 1.0532730700874329\n",
            "Epoch 8, Loss: 0.9772792803001403\n",
            "Epoch 9, Loss: 0.9199367595291138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy計算の関数にも修正が必要です、\n",
        "# モデルの計算に必要な変数XをGPUに移動します。\n",
        "# また、Xを使って計算された変数predsはGPU側に作られるので、\n",
        "# sklearnの関数に渡す際には、cpu()でCPU側に戻す必要があります。\n",
        "# これらについても、自分でコードを書くときには最初から入れておく方が便利です。\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    y_true_list = []\n",
        "    y_pred_list = []\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            ## Xはモデルの計算に必要なのでGPUに移動\n",
        "            X = X.to(device)\n",
        "            outputs = model(X)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            y_true_list.extend(y.numpy())\n",
        "            ## preds.cpu()でCPUに戻す\n",
        "            y_pred_list.extend(preds.cpu().numpy())\n",
        "    acc = accuracy_score(y_true_list, y_pred_list)\n",
        "    return acc\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print('Accuracy on test data: {}'.format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOjx0s7E2Sqj",
        "outputId": "a3d72ae2-05f0-4c25-e3f6-6367c522aae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 0.6627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### 改良型SGDの利用\n",
        "# 「深層学習入門」第2回で学んだ改良型SGDを使用すると、\n",
        "# 学習を安定させたり収束を早めることができます。\n",
        "# 例として、現在最もよく使用されている手法であるAdamを使ってみます。\n",
        "# 使い方は通常のSGDとほとんど同じです。\n",
        "\n",
        "# 乱数シードの設定\n",
        "torch.manual_seed(seed)\n",
        "print('Random seed: {}'.format(seed))\n",
        "\n",
        "# GPUを使えるか判定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: {}'.format(device))\n",
        "\n",
        "# ネットワーク構造の定義\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(28*28, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGDの定義\n",
        "# Adamを使用\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# バッチサイズの指定\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * X_batch.size(0)\n",
        "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
        "    print('Epoch {}, Loss: {}'.format(epoch, avg_loss))\n",
        "\n",
        "\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print('Accuracy on test data: {}'.format(test_acc))\n",
        "\n",
        "# 通常のSGDと比較して学習後のlossが小さくなり、\n",
        "# テストデータに対するaccuracyも高くなりました。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plMsl1py6bIL",
        "outputId": "34728a83-4a99-4dac-c7ea-95f40252b4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed: 42\n",
            "Using device: cpu\n",
            "Epoch 0, Loss: 0.5547939810276031\n",
            "Epoch 1, Loss: 0.38030759690284727\n",
            "Epoch 2, Loss: 0.3435750722980499\n",
            "Epoch 3, Loss: 0.317158204832077\n",
            "Epoch 4, Loss: 0.299096561627388\n",
            "Epoch 5, Loss: 0.28162264620184896\n",
            "Epoch 6, Loss: 0.26694339205265044\n",
            "Epoch 7, Loss: 0.2578299042034149\n",
            "Epoch 8, Loss: 0.24586272143363952\n",
            "Epoch 9, Loss: 0.23642131601333619\n",
            "Accuracy on test data: 0.8747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "【課題】「深層学習入門」第2回において、改良型SGDの1つとしてRMSpropについて学びました。上の例にならって、AdamをRMSpropに変更して、各エポックの損失関数の値を表示してください。学習後のモデルを使って、テストデータに対するaccuracyを計算して表示してください。それ以外の設定は上の例と同じにしてください。\n",
        "\n",
        "通常のSGDと比較して、学習後のlossやテストデータに対するaccuracyはどうなりましたか？コメントの解答欄に書いてください。\n"
      ],
      "metadata": {
        "id": "GKn3z6ijmlvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 乱数シードの設定\n",
        "torch.manual_seed(seed)\n",
        "print('Random seed: {}'.format(seed))\n",
        "\n",
        "# GPUを使えるか判定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: {}'.format(device))\n",
        "\n",
        "# ネットワーク構造の定義\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(28*28, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGDの定義\n",
        "# RMSpropを使用\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "# バッチサイズの指定\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * X_batch.size(0)\n",
        "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
        "    print('Epoch {}, Loss: {}'.format(epoch, avg_loss))\n",
        "\n",
        "\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print('Accuracy on test data: {}'.format(test_acc))\n",
        "\n",
        "#【解答欄】\n",
        "# ここに解答を書いてください。\n",
        "# 通常のSGDと比較して学習後のlossは0.92 --> 0.23と小さくなった。\n",
        "# テストデータに対するaccuracyは0.66--> 0.88と高くなった。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtz9RWbCn2Xk",
        "outputId": "e646592f-5a3a-46cf-ab8e-2cd6f3f2b3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed: 42\n",
            "Using device: cpu\n",
            "Epoch 0, Loss: 0.5442892530822754\n",
            "Epoch 1, Loss: 0.3861286721992493\n",
            "Epoch 2, Loss: 0.3439986678981781\n",
            "Epoch 3, Loss: 0.31688272253990174\n",
            "Epoch 4, Loss: 0.2948024749755859\n",
            "Epoch 5, Loss: 0.2783399308347702\n",
            "Epoch 6, Loss: 0.2657232114624977\n",
            "Epoch 7, Loss: 0.2526134718465805\n",
            "Epoch 8, Loss: 0.24256887273788452\n",
            "Epoch 9, Loss: 0.23288747489452363\n",
            "Accuracy on test data: 0.8775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### バリデーションデータの利用\n",
        "# 深層学習では、バリデーションデータを使用して過学習の検知やハイパラの調整を行います。\n",
        "# 以下の例では、学習の各エポックでバリデーションデータに対する損失を評価しています。\n",
        "# また、学習のエポック数を20に増やしています。\n",
        "\n",
        "# 乱数シードの設定\n",
        "torch.manual_seed(seed)\n",
        "print('Random seed: {}'.format(seed))\n",
        "\n",
        "# GPUを使えるか判定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: {}'.format(device))\n",
        "\n",
        "# ネットワーク構造の定義\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(28*28, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGDの定義\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# dataloaderの作成\n",
        "# 学習データ用、バリデーションデータ用、テストデータ用をそれぞれ作成\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# 学習ループ\n",
        "# 20エポック回す\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * X_batch.size(0)\n",
        "    # 学習データに対する損失\n",
        "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
        "    # バリデーションデータに対する損失\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad(): # バリデーションデータの勾配は不要なので自動微分をOFF\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            outputs = model(X_val)\n",
        "            loss = criterion(outputs, y_val)\n",
        "            val_loss += loss.item() * X_val.size(0)\n",
        "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "    # 学習データ、バリデーションデータに対する損失を表示\n",
        "    print('Epoch {}, Train Loss: {}, Val Loss: {}'.format(epoch, avg_loss, avg_val_loss))\n",
        "\n",
        "\n",
        "# テストデータに対するaccuracy\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print('Accuracy on test data: {}'.format(test_acc))\n",
        "\n",
        "# 学習中、学習データに対する損失は下がり続けています。\n",
        "# 一方、バリデーションデータに対する損失はEpoch 9あたりから低下しなくなり\n",
        "# それ以降のEpochでは上昇していく傾向にあります。\n",
        "# このことから過学習が起こっていると考えられます。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1NJhSEb6ywf",
        "outputId": "8e325121-4782-4967-ec45-f02139083908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed: 42\n",
            "Using device: cpu\n",
            "Epoch 0, Train Loss: 0.5547939810276031, Val Loss: 0.4049241940021515\n",
            "Epoch 1, Train Loss: 0.38167404633522034, Val Loss: 0.36330958909988403\n",
            "Epoch 2, Train Loss: 0.34457733348846437, Val Loss: 0.36022012333869935\n",
            "Epoch 3, Train Loss: 0.32084663261413576, Val Loss: 0.33543130996227266\n",
            "Epoch 4, Train Loss: 0.2970120664358139, Val Loss: 0.3201426437497139\n",
            "Epoch 5, Train Loss: 0.28167706802129744, Val Loss: 0.3146257018685341\n",
            "Epoch 6, Train Loss: 0.270006624712944, Val Loss: 0.3456172767162323\n",
            "Epoch 7, Train Loss: 0.2583450923013687, Val Loss: 0.315134344291687\n",
            "Epoch 8, Train Loss: 0.2433577919149399, Val Loss: 0.32200641182661055\n",
            "Epoch 9, Train Loss: 0.23574478466033935, Val Loss: 0.2955359766721726\n",
            "Epoch 10, Train Loss: 0.2237887127685547, Val Loss: 0.3579214657783508\n",
            "Epoch 11, Train Loss: 0.21684333181381227, Val Loss: 0.30651279792785646\n",
            "Epoch 12, Train Loss: 0.2057379571533203, Val Loss: 0.3087021566510201\n",
            "Epoch 13, Train Loss: 0.20136573429107665, Val Loss: 0.3115143236160278\n",
            "Epoch 14, Train Loss: 0.1957837612724304, Val Loss: 0.3146034786105156\n",
            "Epoch 15, Train Loss: 0.18399334935188294, Val Loss: 0.3375514503717422\n",
            "Epoch 16, Train Loss: 0.17824164009928703, Val Loss: 0.35439375054836275\n",
            "Epoch 17, Train Loss: 0.17357224950909614, Val Loss: 0.36329278917312624\n",
            "Epoch 18, Train Loss: 0.16422926172733307, Val Loss: 0.3316493301987648\n",
            "Epoch 19, Train Loss: 0.15746857728242875, Val Loss: 0.3932093644857407\n",
            "Accuracy on test data: 0.8789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### ドロップアウトの導入\n",
        "# 過学習を防ぐための手法として、\n",
        "#「深層学習入門」第3回で学んだドロップアウトがあります。\n",
        "# ネットワーク構造の定義においてnn.Dropout()を入れることで、\n",
        "# その1つ前の層に対してドロップアウトを行うことができます。\n",
        "\n",
        "# 乱数シードの設定\n",
        "torch.manual_seed(seed)\n",
        "print('Random seed: {}'.format(seed))\n",
        "\n",
        "# GPUを使えるか判定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: {}'.format(device))\n",
        "\n",
        "# ネットワーク構造の定義\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(28*28, 256),\n",
        "    nn.ReLU(),             # このReLUの結果がランダムに0になる\n",
        "    nn.Dropout(0.5),       # ドロップアウト(確率0.5)\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGDの定義\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# dataloaderの作成\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * X_batch.size(0)\n",
        "    # 学習データに対する損失\n",
        "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
        "    # バリデーションデータに対する損失\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            outputs = model(X_val)\n",
        "            loss = criterion(outputs, y_val)\n",
        "            val_loss += loss.item() * X_val.size(0)\n",
        "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "    # 学習データ、バリデーションデータに対する損失を表示\n",
        "    print('Epoch {}, Train Loss: {}, Val Loss: {}'.format(epoch, avg_loss, avg_val_loss))\n",
        "\n",
        "\n",
        "# テストデータに対するaccuracy\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print('Accuracy on test data: {}'.format(test_acc))\n",
        "\n",
        "# 学習の後半、バリデーションデータの損失が上昇する傾向が弱くなりました。\n",
        "# このことから、過学習をある程度低減できていると言えそうです。\n",
        "# 一方、学習データの損失は学習によって低下しているものの、ドロップアウトなしの場合より高くなっています。\n",
        "# このように、ドロップアウトを行うと学習データへのフィッティングは悪くなることが多いです。\n",
        "# テストデータに対するaccuracyは0.88であり、\n",
        "# ドロップアウトなしの場合とあまり変わりません。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-5MHP9w8R-6",
        "outputId": "de5823c4-ac3c-4e7a-811a-b5bc9dbe16f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed: 42\n",
            "Using device: cpu\n",
            "Epoch 0, Train Loss: 0.6300883201694488, Val Loss: 0.4444670174360275\n",
            "Epoch 1, Train Loss: 0.4541747616481781, Val Loss: 0.3850455300807953\n",
            "Epoch 2, Train Loss: 0.41776053621292114, Val Loss: 0.36552734005451204\n",
            "Epoch 3, Train Loss: 0.3969711669826508, Val Loss: 0.3804621128797531\n",
            "Epoch 4, Train Loss: 0.3824440495014191, Val Loss: 0.356860085439682\n",
            "Epoch 5, Train Loss: 0.36837677913665773, Val Loss: 0.34066004927158355\n",
            "Epoch 6, Train Loss: 0.35749769211292265, Val Loss: 0.3354764726638794\n",
            "Epoch 7, Train Loss: 0.34902029572486876, Val Loss: 0.32951362130641937\n",
            "Epoch 8, Train Loss: 0.3418429273509979, Val Loss: 0.334777814912796\n",
            "Epoch 9, Train Loss: 0.3365267326259613, Val Loss: 0.3182085011959076\n",
            "Epoch 10, Train Loss: 0.32978487602233886, Val Loss: 0.31861111223697663\n",
            "Epoch 11, Train Loss: 0.32410105884552004, Val Loss: 0.32227389376163484\n",
            "Epoch 12, Train Loss: 0.3189131157398224, Val Loss: 0.3129680426597595\n",
            "Epoch 13, Train Loss: 0.3142433122110367, Val Loss: 0.30729211287498476\n",
            "Epoch 14, Train Loss: 0.3076850533103943, Val Loss: 0.31792491850852966\n",
            "Epoch 15, Train Loss: 0.30496947799682617, Val Loss: 0.3042511610984802\n",
            "Epoch 16, Train Loss: 0.30148562336444856, Val Loss: 0.3160268943786621\n",
            "Epoch 17, Train Loss: 0.29766608130931854, Val Loss: 0.3251140099287033\n",
            "Epoch 18, Train Loss: 0.29460679401397705, Val Loss: 0.30409685065746306\n",
            "Epoch 19, Train Loss: 0.28884796716690064, Val Loss: 0.30280782871246337\n",
            "Accuracy on test data: 0.8822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### model.train(), model.eval()\n",
        "# ニューラルネットワークのレイヤーの中には、学習時と予測時で処理の異なるものがあります。\n",
        "# 例えば、ドロップアウトは、\n",
        "# 学習時にはユニットの出力を一定の確率で0にする(ユニットを削除する)処理を行いますが、\n",
        "# 予測時には全てのユニットを使用して出力を計算します。\n",
        "# 今まで使ってきたmodel.train(), model.eval()は、\n",
        "# このような学習時と予測時の動作の切り替えを行う機能を持っています。\n",
        "# 自分でコードを書くときには、いつも入れておくのが安全です。"
      ],
      "metadata": {
        "id": "KO_c97A6Bm4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### バッチ正規化の導入\n",
        "# 学習の数値的安定性を高めるための手法として、\n",
        "# 「深層学習入門」第3回で学んだバッチ正規化があります。\n",
        "# ネットワーク構造の定義においてnn.BatchNorm1d()を入れることで、\n",
        "# その1つ前の層の出力に対してバッチ正規化を行うことができます。\n",
        "\n",
        "# 乱数シードの設定\n",
        "torch.manual_seed(seed)\n",
        "print('Random seed: {}'.format(seed))\n",
        "\n",
        "# GPUを使えるか判定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: {}'.format(device))\n",
        "\n",
        "# ネットワーク構造の定義\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(28*28, 256),\n",
        "    nn.BatchNorm1d(256),   # 1つ前のLinear層の出力をバッチ正規化\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),       # ドロップアウトも使用\n",
        "    nn.Linear(256, 128),\n",
        "    nn.BatchNorm1d(128),   # 1つ前のLinear層の出力をバッチ正規化\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGDの定義\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# dataloaderの作成\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * X_batch.size(0)\n",
        "    # 学習データに対する損失\n",
        "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
        "    # バリデーションデータに対する損失\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            outputs = model(X_val)\n",
        "            loss = criterion(outputs, y_val)\n",
        "            val_loss += loss.item() * X_val.size(0)\n",
        "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "    # 学習データ、バリデーションデータに対する損失を表示\n",
        "    print('Epoch {}, Train Loss: {}, Val Loss: {}'.format(epoch, avg_loss, avg_val_loss))\n",
        "\n",
        "\n",
        "# テストデータに対するaccuracy\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print('Accuracy on test data: {}'.format(test_acc))\n",
        "\n",
        "\n",
        "# バリデーションデータの損失は上昇しておらず、\n",
        "# 学習データの損失もドロップアウトのみの場合より小さくなりました。\n",
        "# 一方、テストデータに対するaccuracyは0.89であり、\n",
        "# 先程までのモデルとあまり変わりません。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1CUT4ERIZuc",
        "outputId": "33a82abb-042a-42f0-eb41-3f0cbdd28003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed: 42\n",
            "Using device: cpu\n",
            "Epoch 0, Train Loss: 0.5516651962661743, Val Loss: 0.4354554348945618\n",
            "Epoch 1, Train Loss: 0.42528948493003843, Val Loss: 0.3720967957496643\n",
            "Epoch 2, Train Loss: 0.3919573538780212, Val Loss: 0.35312238719463346\n",
            "Epoch 3, Train Loss: 0.36853630029678347, Val Loss: 0.33329965550899504\n",
            "Epoch 4, Train Loss: 0.35640258535385133, Val Loss: 0.3269489110708237\n",
            "Epoch 5, Train Loss: 0.3407185674476624, Val Loss: 0.31668895487785337\n",
            "Epoch 6, Train Loss: 0.33065053926467897, Val Loss: 0.3154347938299179\n",
            "Epoch 7, Train Loss: 0.3187043284416199, Val Loss: 0.3119892119407654\n",
            "Epoch 8, Train Loss: 0.31296337824821474, Val Loss: 0.3095575761079788\n",
            "Epoch 9, Train Loss: 0.30212648568153383, Val Loss: 0.3058978836774826\n",
            "Epoch 10, Train Loss: 0.2952130706977844, Val Loss: 0.304269549369812\n",
            "Epoch 11, Train Loss: 0.2919199090385437, Val Loss: 0.2990282628059387\n",
            "Epoch 12, Train Loss: 0.28302521164894107, Val Loss: 0.30072348308563235\n",
            "Epoch 13, Train Loss: 0.27724399882793427, Val Loss: 0.29377342348098756\n",
            "Epoch 14, Train Loss: 0.27273278297424314, Val Loss: 0.29303742055892945\n",
            "Epoch 15, Train Loss: 0.2664666133594513, Val Loss: 0.29463255039453506\n",
            "Epoch 16, Train Loss: 0.26338332154273986, Val Loss: 0.29744345593452454\n",
            "Epoch 17, Train Loss: 0.2614627188205719, Val Loss: 0.29653908684253694\n",
            "Epoch 18, Train Loss: 0.25561606399536135, Val Loss: 0.283126673913002\n",
            "Epoch 19, Train Loss: 0.25248633458137515, Val Loss: 0.2926855079650879\n",
            "Accuracy on test data: 0.8916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "【課題】上の例にならって、ドロップアウトとバッチ正規化を導入したネットワークに対して、ドロップアウト確率を0.3に変更して、各エポックにおける学習データの損失、バリデーションデータの損失を表示してください。また、テストデータに対するaccuracyを計算して表示してください。それ以外の設定は上の例と同じにしてください。"
      ],
      "metadata": {
        "id": "u2_qZOCTOnoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 乱数シードの設定\n",
        "torch.manual_seed(seed)\n",
        "print('Random seed: {}'.format(seed))\n",
        "\n",
        "# GPUを使えるか判定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: {}'.format(device))\n",
        "\n",
        "# ネットワーク構造の定義\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(28*28, 256),\n",
        "    nn.BatchNorm1d(256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),       # ドロップアウト確率を0.3に設定\n",
        "    nn.Linear(256, 128),\n",
        "    nn.BatchNorm1d(128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGDの定義\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# dataloaderの作成\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * X_batch.size(0)\n",
        "    # 学習データに対する損失\n",
        "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
        "    # バリデーションデータに対する損失\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            outputs = model(X_val)\n",
        "            loss = criterion(outputs, y_val)\n",
        "            val_loss += loss.item() * X_val.size(0)\n",
        "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "    # 学習データ、バリデーションデータに対する損失を表示\n",
        "    print('Epoch {}, Train Loss: {}, Val Loss: {}'.format(epoch, avg_loss, avg_val_loss))\n",
        "\n",
        "\n",
        "# テストデータに対するaccuracy\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print('Accuracy on test data: {}'.format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1f5WCIfPWM7",
        "outputId": "b9585523-53af-4322-e51c-722fb481f2d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed: 42\n",
            "Using device: cpu\n",
            "Epoch 0, Train Loss: 0.5094331258392334, Val Loss: 0.4274532521724701\n",
            "Epoch 1, Train Loss: 0.3870658652687073, Val Loss: 0.3544882797718048\n",
            "Epoch 2, Train Loss: 0.35022201350212095, Val Loss: 0.3372200974225998\n",
            "Epoch 3, Train Loss: 0.3259115677833557, Val Loss: 0.32081438987255095\n",
            "Epoch 4, Train Loss: 0.3126559733772278, Val Loss: 0.3045757827281952\n",
            "Epoch 5, Train Loss: 0.29747344458580016, Val Loss: 0.30528345670700074\n",
            "Epoch 6, Train Loss: 0.28487977198600767, Val Loss: 0.3029666917085648\n",
            "Epoch 7, Train Loss: 0.27288981605529783, Val Loss: 0.3145903688907623\n",
            "Epoch 8, Train Loss: 0.26177658328056336, Val Loss: 0.31448354811668394\n",
            "Epoch 9, Train Loss: 0.25501125589370727, Val Loss: 0.3011355150103569\n",
            "Epoch 10, Train Loss: 0.24616671602249146, Val Loss: 0.2942640570163727\n",
            "Epoch 11, Train Loss: 0.23835686042785645, Val Loss: 0.31115437541007995\n",
            "Epoch 12, Train Loss: 0.2333948631000519, Val Loss: 0.30501475112438203\n",
            "Epoch 13, Train Loss: 0.22427219120025635, Val Loss: 0.2882071568548679\n",
            "Epoch 14, Train Loss: 0.2192020021533966, Val Loss: 0.3072242260813713\n",
            "Epoch 15, Train Loss: 0.21189329986572267, Val Loss: 0.3112948826611042\n",
            "Epoch 16, Train Loss: 0.20945992488384246, Val Loss: 0.3004763263642788\n",
            "Epoch 17, Train Loss: 0.20572127329349518, Val Loss: 0.3085060292303562\n",
            "Epoch 18, Train Loss: 0.20212241517543791, Val Loss: 0.2959970251441002\n",
            "Epoch 19, Train Loss: 0.19306479614257813, Val Loss: 0.29504477261304857\n",
            "Accuracy on test data: 0.8912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### 畳み込みニューラルネットワーク(CNN)\n",
        "# CNNを使用する場合も、多層パーセプトロンと同様に、\n",
        "# レイヤーを部品のように組み合わせてネットワーク構造を定義します。\n",
        "# torch.nnに畳み込みやプーリングのクラスが用意されています。\n",
        "\n",
        "# nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "# 画像のような2次元データに対して畳み込み演算を行うレイヤー。\n",
        "# 入力のshapeは3次元(チャネル, 高さ, 幅)\n",
        "# 主な引数として、入力チャネル数、出力チャネル数(フィルタ数)、\n",
        "# カーネル(フィルタ)のサイズ、ストライド、パディングするピクセル数などを指定。\n",
        "\n",
        "# nn.MaxPool2d(kernel_size, stride)\n",
        "# 画像のような2次元データに対して最大プーリングを行うレイヤー。\n",
        "# 入力のshapeは3次元(チャネル, 高さ, 幅)\n",
        "# 主な引数として、カーネル(フィルタ)のサイズなどを指定。\n",
        "\n",
        "# ネットワーク構造の定義\n",
        "# Fashion MNISTの画像データのshapeは(1, 28, 28)なので、\n",
        "# 最初の畳み込み層のin_channelsは1とします。\n",
        "model = nn.Sequential(\n",
        "    # 畳み込み＆プーリング1\n",
        "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    # 畳み込み＆プーリング2\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    # nn.Flatten()で1次元に変換して全結合層へ\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64 * 7 * 7, 128), # 入力のユニット数に注意\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "ICYb9iQTbxe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 全結合層に繋げる部分のユニット数の指定には注意が必要です。\n",
        "# カーネルサイズ、ストライド、パディングなどを考慮して、\n",
        "# shapeがどのように変化していくか考える必要があります。\n",
        "\n",
        "# ネットワーク構造の定義\n",
        "model = nn.Sequential(\n",
        "    # 入力画像は(1, 28, 28)\n",
        "\n",
        "    # 畳み込み＆プーリング1\n",
        "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "    # フィルタ数32で畳み込みしたのでチャネル数は32に増える。\n",
        "    # カーネルサイズ3、パディング1なので、高さ、幅は変わらない。\n",
        "    # そのためshapeは(32, 28, 28)となる。\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    # 最大プーリングではチャネル数は変わらない。\n",
        "    # カーネルサイズ2、ストライド2なので、高さ、幅は半分になる。\n",
        "    # そのためshapeは(32, 14, 14)となる。\n",
        "\n",
        "    # 畳み込み＆プーリング2\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "    # フィルタ数64で畳み込みしたのでチャネル数は64に増える。\n",
        "    # カーネルサイズ3、パディング1なので、高さ、幅は変わらない。\n",
        "    # そのためshapeは(64, 14, 14)となる。\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    # 最大プーリングではチャネル数は変わらない。\n",
        "    # カーネルサイズ2、ストライド2なので、高さ、幅は半分になる。\n",
        "    # そのためshapeは(64, 7, 7)となる。\n",
        "\n",
        "    # (64, 7, 7)を1次元に変換\n",
        "    nn.Flatten(),\n",
        "    # そのため入力のユニット数は64 * 7 * 7となる\n",
        "    nn.Linear(64 * 7 * 7, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "fEQnR0Hxh4eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ネットワークを定義した後の流れは、これまでと同様です。\n",
        "# CNNの計算は、GPUを使用するとかなり高速化します。\n",
        "# CPUしか使えないと計算は重いです。\n",
        "\n",
        "# 乱数シードの設定\n",
        "torch.manual_seed(seed)\n",
        "print('Random seed: {}'.format(seed))\n",
        "\n",
        "# GPUを使えるか判定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: {}'.format(device))\n",
        "\n",
        "# ネットワーク構造の定義\n",
        "model = nn.Sequential(\n",
        "    # 畳み込み＆プーリング1\n",
        "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    # 畳み込み＆プーリング2\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    # 全結合層\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64 * 7 * 7, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGDの定義\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# dataloaderの作成\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * X_batch.size(0)\n",
        "    # 学習データに対する損失\n",
        "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
        "    # バリデーションデータに対する損失\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            outputs = model(X_val)\n",
        "            loss = criterion(outputs, y_val)\n",
        "            val_loss += loss.item() * X_val.size(0)\n",
        "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "    # 学習データ、バリデーションデータに対する損失を表示\n",
        "    print('Epoch {}, Train Loss: {}, Val Loss: {}'.format(epoch, avg_loss, avg_val_loss))\n",
        "\n",
        "\n",
        "# テストデータに対するaccuracy\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print('Accuracy on test data: {}'.format(test_acc))\n",
        "\n",
        "# テストデータに対するaccuracyは0.91となり、多層パーセプトロンより高くなりました。\n",
        "# 一方、学習中にバリデーションデータの損失が上昇しており、過学習の傾向が見られます。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBLsXPFoEFtd",
        "outputId": "72b8dccc-ee65-4ab1-f913-1bb2901ca032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed: 42\n",
            "Using device: cpu\n",
            "Epoch 0, Train Loss: 0.48517267895698546, Val Loss: 0.35779689018726346\n",
            "Epoch 1, Train Loss: 0.30864957421302797, Val Loss: 0.3014959420681\n",
            "Epoch 2, Train Loss: 0.2609649342727661, Val Loss: 0.25917087880373\n",
            "Epoch 3, Train Loss: 0.23208506774902343, Val Loss: 0.256802592253685\n",
            "Epoch 4, Train Loss: 0.20662740722894668, Val Loss: 0.23854945868253707\n",
            "Epoch 5, Train Loss: 0.18586748372793196, Val Loss: 0.22932424560785294\n",
            "Epoch 6, Train Loss: 0.16700352699041365, Val Loss: 0.21524789853096007\n",
            "Epoch 7, Train Loss: 0.14800080575227736, Val Loss: 0.21651316295266151\n",
            "Epoch 8, Train Loss: 0.13017285982608795, Val Loss: 0.2172696790188551\n",
            "Epoch 9, Train Loss: 0.11685897644519806, Val Loss: 0.21567643413692714\n",
            "Epoch 10, Train Loss: 0.10030245037555695, Val Loss: 0.22682637152187526\n",
            "Epoch 11, Train Loss: 0.0876195889866352, Val Loss: 0.23060812353268265\n",
            "Epoch 12, Train Loss: 0.07490985225498677, Val Loss: 0.24433898410275579\n",
            "Epoch 13, Train Loss: 0.06369420378446579, Val Loss: 0.24955607781810685\n",
            "Epoch 14, Train Loss: 0.05489784328609705, Val Loss: 0.3081068353943527\n",
            "Epoch 15, Train Loss: 0.047771578515172004, Val Loss: 0.29511381526067854\n",
            "Epoch 16, Train Loss: 0.042110680795907976, Val Loss: 0.32833991744800006\n",
            "Epoch 17, Train Loss: 0.035417500110119585, Val Loss: 0.32645004110210574\n",
            "Epoch 18, Train Loss: 0.030252721568904815, Val Loss: 0.3537266995523125\n",
            "Epoch 19, Train Loss: 0.03015864250406623, Val Loss: 0.3649000874192454\n",
            "Accuracy on test data: 0.918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### CNNにおけるドロップアウト、バッチ正規化\n",
        "# 過学習の低減、学習の安定性のために、\n",
        "# CNNにもドロップアウトとバッチ正規化を導入します。\n",
        "# nn.Conv2d()に対してバッチ正規化を行う場合、nn.BatchNorm2d()を使用します。\n",
        "# nn.BatchNorm2d()の引数にはチャネル数を指定します。\n",
        "\n",
        "# 乱数シードの設定\n",
        "torch.manual_seed(seed)\n",
        "print('Random seed: {}'.format(seed))\n",
        "\n",
        "# GPUを使えるか判定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: {}'.format(device))\n",
        "\n",
        "# ネットワーク構造の定義\n",
        "model = nn.Sequential(\n",
        "    # 畳み込み＆プーリング1\n",
        "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "    nn.BatchNorm2d(32),  # バッチ正規化\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.25),    # ドロップアウト\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    # 畳み込み＆プーリング2\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "    nn.BatchNorm2d(64),  # バッチ正規化\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.25),    # ドロップアウト\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    # 全結合層\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64 * 7 * 7, 128),\n",
        "    nn.BatchNorm1d(128), # ここは1次元なのでBatchNorm1d()\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),     # ドロップアウト\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGDの定義\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# dataloaderの指定\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * X_batch.size(0)\n",
        "    # 学習データに対する損失\n",
        "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
        "    # バリデーションデータに対する損失\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            outputs = model(X_val)\n",
        "            loss = criterion(outputs, y_val)\n",
        "            val_loss += loss.item() * X_val.size(0)\n",
        "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "    # 学習データ、バリデーションデータに対する損失を表示\n",
        "    print('Epoch {}, Train Loss: {}, Val Loss: {}'.format(epoch, avg_loss, avg_val_loss))\n",
        "\n",
        "\n",
        "# テストデータに対するaccuracy\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print('Accuracy on test data: {}'.format(test_acc))\n",
        "\n",
        "# 学習中にバリデーションデータの損失が上昇しなくなり、過学習を低減できました。\n",
        "# テストデータに対するaccuracyは0.93となり、これまでで一番高くなりました。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPfjncXzGLh-",
        "outputId": "f9c3cc03-34f9-4cf3-f878-bf8e592853f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed: 42\n",
            "Using device: cpu\n",
            "Epoch 0, Train Loss: 0.44002080263137816, Val Loss: 0.3099728805541992\n",
            "Epoch 1, Train Loss: 0.3051741741466522, Val Loss: 0.2905934884309769\n",
            "Epoch 2, Train Loss: 0.2705926326847076, Val Loss: 0.26118716233968736\n",
            "Epoch 3, Train Loss: 0.24988129682540894, Val Loss: 0.2632207538843155\n",
            "Epoch 4, Train Loss: 0.23551233954906464, Val Loss: 0.26379272607564924\n",
            "Epoch 5, Train Loss: 0.2225722536468506, Val Loss: 0.22347612277269363\n",
            "Epoch 6, Train Loss: 0.21087713823795318, Val Loss: 0.23192704473733902\n",
            "Epoch 7, Train Loss: 0.2031723307132721, Val Loss: 0.22172870951890947\n",
            "Epoch 8, Train Loss: 0.19717631232738494, Val Loss: 0.23040133513212205\n",
            "Epoch 9, Train Loss: 0.18527552570819855, Val Loss: 0.2101199106782675\n",
            "Epoch 10, Train Loss: 0.18185719636678696, Val Loss: 0.20419561318159105\n",
            "Epoch 11, Train Loss: 0.1740176070690155, Val Loss: 0.20991575306653976\n",
            "Epoch 12, Train Loss: 0.1694577922153473, Val Loss: 0.21016493985652923\n",
            "Epoch 13, Train Loss: 0.16794961712360382, Val Loss: 0.20705614595413208\n",
            "Epoch 14, Train Loss: 0.16160988810300828, Val Loss: 0.22204808460474015\n",
            "Epoch 15, Train Loss: 0.15733404253840447, Val Loss: 0.20291364351511001\n",
            "Epoch 16, Train Loss: 0.15663370139360427, Val Loss: 0.19797515159845352\n",
            "Epoch 17, Train Loss: 0.14722062643527983, Val Loss: 0.20466117574572562\n",
            "Epoch 18, Train Loss: 0.14994908455848693, Val Loss: 0.20412273854613305\n",
            "Epoch 19, Train Loss: 0.13886130426168442, Val Loss: 0.19331281521320343\n",
            "Accuracy on test data: 0.9263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "【課題】上の例を元に、チャネル数、カーネルサイズ、パティングを変更したCNNを作成して、学習後の損失関数の値を表示してください。各エポックにおける学習データの損失、バリデーションデータの損失を表示してください。また、テストデータに対するaccuracyを計算して表示してください。それ以外の設定は上の例と同じにしてください。\n",
        "\n",
        "CNNの構造は自由に設定していいですが、エラーが出る設定や、学習で損失が全く低下しない設定は不可とします。ネットワークを大きくしすぎると計算時間が非常に長くなるので注意してください。\n"
      ],
      "metadata": {
        "id": "QgRJOGLSoA2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "vgvF2Hc0_Dzt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}